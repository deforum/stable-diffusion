{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c442uQJ_gUgy"
      },
      "source": [
        "# **Deforum Stable Diffusion**\n",
        "[Stable Diffusion](https://github.com/CompVis/stable-diffusion) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer and the [Stability.ai](https://stability.ai/) Team\n",
        "\n",
        "Notebook by [deforum](https://twitter.com/deforum_art)\n",
        "\n",
        "Changes by [Thibaudz](https://twitter.com/thibaudz): \n",
        "\n",
        "- the seed is incremented by 1 between each iteration and save in the name of each file\n",
        "- grid without space between image\n",
        "- autoset ddim_eta to 0 when plms is checked\n",
        "- f (downsampling factor) default to 8 (so the size of the image are really the one filled and not x2)\n",
        "- skip env setup checkbox\n",
        "- multi prompt\n",
        "- more samplers (\"ddim\",\"plms\",\"k_lms\",\"dpm_2\",\"dpm_2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Setup"
      ],
      "metadata": {
        "id": "wF1maFSNL1Lh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g-f7cQmf2Nt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **GPU**\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRNl2mfepEIe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **Env Setup**\n",
        "\n",
        "skip_env_setup = True #@param {type:\"boolean\"}\n",
        "\n",
        "if not skip_env_setup:\n",
        "  #@markdown Runtime > Restart Runtime\n",
        "  %pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "  %pip install omegaconf==2.1.1 einops==0.3.0 pytorch-lightning==1.4.2 torchmetrics==0.6.0 torchtext==0.2.3 transformers==4.19.2 kornia==0.6\n",
        "  !git clone https://github.com/deforum/stable-diffusion\n",
        "  #%pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "  %pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "  %pip install git+https://github.com/crowsonkb/k-diffusion/\n",
        "  print(\"ici\")\n",
        "else:\n",
        "  print(\"Setup env [SKIPPED]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzU1bmrigJJB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **Local Path Variables**\n",
        "models_path = \"models/ldm/stable1p4b\" #@param {type:\"string\"}\n",
        "output_path = \"/mnt/c/Users/Thiba/Downloads/output_colab\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Google Drive Path Variables (Optional)**\n",
        "mount_google_drive = False #@param {type:\"boolean\"}\n",
        "force_remount = False\n",
        "\n",
        "if mount_google_drive:\n",
        "  from google.colab import drive\n",
        "  try:\n",
        "    drive_path = \"/content/drive\"\n",
        "    drive.mount(drive_path,force_remount=force_remount)\n",
        "    models_path = \"/content/drive/MyDrive/AI/models\" #@param {type:\"string\"}\n",
        "    output_path = \"/content/drive/MyDrive/AI/StableDiffusion\" #@param {type:\"string\"}\n",
        "  except:\n",
        "    print(\"...error mounting drive or with drive path variables\")\n",
        "    print(\"...reverting to default path variables\")\n",
        "    models_path = \"/content/models\"\n",
        "    output_path = \"/content/output\"\n",
        "\n",
        "!mkdir -p $models_path\n",
        "!mkdir -p $output_path\n",
        "\n",
        "print(f\"models_path: {models_path}\")\n",
        "print(f\"output_path: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FVWjI1vclPO"
      },
      "outputs": [],
      "source": [
        "#@markdown **Python Definitions**\n",
        "import json\n",
        "from IPython import display\n",
        "#from tqdm.notebook import tqdm, trange\n",
        "\n",
        "#from google.colab import output\n",
        "#output.enable_custom_widget_manager()\n",
        "\n",
        "import sys, os\n",
        "import argparse, glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange, repeat\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from contextlib import contextmanager, nullcontext\n",
        "\n",
        "sys.path.append('./stable-diffusion/')\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "import k_diffusion as K\n",
        "import accelerate\n",
        "\n",
        "#import for animation\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "from glob import glob\n",
        "from resize_right import resize\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def get_output_folder(output_path,batch_folder=None):\n",
        "  yearMonth = time.strftime('%Y-%m/')\n",
        "  out_path = output_path+\"/\"+yearMonth\n",
        "  if batch_folder != \"\":\n",
        "    out_path += batch_folder\n",
        "    if out_path[-1] != \"/\":\n",
        "      out_path += \"/\"\n",
        "  os.makedirs(out_path, exist_ok=True)\n",
        "  return out_path\n",
        "\n",
        "def load_img(path,w,h):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    #w, h = image.size\n",
        "    image = TF.resize(image, min(w, h, *image.size), T.InterpolationMode.LANCZOS)\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float16) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "def run(params):\n",
        "\n",
        "    # outpath\n",
        "    os.makedirs(params[\"outdir\"], exist_ok=True)\n",
        "    outpath = params[\"outdir\"]\n",
        "\n",
        "    # timestring\n",
        "    timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "    # random seed\n",
        "    if params[\"seed\"] == -1:\n",
        "      local_seed = np.random.randint(0,4294967295)\n",
        "    else:\n",
        "      local_seed = params[\"seed\"]\n",
        "\n",
        "    # save/append settings\n",
        "    if params[\"save_settings\"] and params[\"filename\"] is None:\n",
        "      filename = f\"{timestring}_settings.txt\"\n",
        "      assert not os.path.isfile(f\"{outpath}{filename}\")\n",
        "      params[\"filename\"] = f\"{timestring}_settings.txt\"\n",
        "      params[\"batch_seeds\"] = [local_seed]\n",
        "      with open(f\"{outpath}{filename}\", \"w+\") as f:\n",
        "        json.dump(params, f, ensure_ascii=False, indent=4)\n",
        "    elif params[\"save_settings\"] and params[\"filename\"] is not None:\n",
        "      filename = params[\"filename\"]\n",
        "      with open(f\"{outpath}{filename}\") as f:\n",
        "        params = json.load(f)\n",
        "      params[\"batch_seeds\"] += [local_seed]\n",
        "      with open(f\"{outpath}{filename}\", \"w+\") as f:\n",
        "        json.dump(params, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # load settings\n",
        "\n",
        "    accelerator = accelerate.Accelerator()\n",
        "    device = accelerator.device\n",
        "    seeds = torch.randint(-2 ** 63, 2 ** 63 - 1, [accelerator.num_processes])\n",
        "    torch.manual_seed(seeds[accelerator.process_index].item())\n",
        "\n",
        "    # seed\n",
        "    seed_everything(local_seed)\n",
        "\n",
        "\n",
        "    # plms\n",
        "    if params[\"sampler\"]==\"plms\":\n",
        "        params[\"ddim_eta\"] = 0\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    model_wrap = K.external.CompVisDenoiser(model)\n",
        "    sigma_min, sigma_max = model_wrap.sigmas[0].item(), model_wrap.sigmas[-1].item()\n",
        "\n",
        "    batch_size = params[\"n_samples\"]\n",
        "    n_rows = params[\"n_rows\"] if params[\"n_rows\"] > 0 else batch_size\n",
        "\n",
        "    if params[\"key_frames\"]:\n",
        "      angle_series = get_inbetweens(parse_key_frames(params[\"angle\"]))\n",
        "      zoom_series = get_inbetweens(parse_key_frames(params[\"zoom\"]))\n",
        "      translation_x_series = get_inbetweens(parse_key_frames(params[\"translation_x\"]))\n",
        "      translation_y_series = get_inbetweens(parse_key_frames(params[\"translation_y\"]))\n",
        "\n",
        "    #if not params[\"from_file\"]:\n",
        "    #    prompt = params[\"prompt\"]\n",
        "    #    assert prompt is not None\n",
        "    #    data = [batch_size * [prompt]]\n",
        "    #else:\n",
        "    #    print(f\"reading prompts from {from_file}\")\n",
        "    #    with open(from_file, \"r\") as f:\n",
        "    #        data = f.read().splitlines()\n",
        "    #        data = list(chunk(data, batch_size))\n",
        "\n",
        "    print(params[\"prompts\"])\n",
        "\n",
        "    data = list(chunk(params[\"prompts\"], batch_size))\n",
        "\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    #base_count = len(os.listdir(sample_path))\n",
        "    base_count = 0\n",
        "\n",
        "    start_code = None\n",
        "    print(params)\n",
        "    if params[\"animation_mode\"] == \"None\":\n",
        "      params[\"max_frames\"] = 1\n",
        "    else:\n",
        "      A=1 #params[\"max_frames\"] = 24\n",
        "\n",
        "    for frame_num in range(params[\"max_frames\"]):   \n",
        "\n",
        "      #Animation\n",
        "      if  params[\"animation_mode\"] == \"Video Input\":\n",
        "        display.clear_output(wait=True)\n",
        "        #if not video_init_seed_continuity:\n",
        "        #  seed += 1\n",
        "        params[\"use_init\"] = True\n",
        "        params[\"init_image\"] = f'{params[\"videoFramesFolder\"]}/{frame_num+1:04}.jpg'\n",
        "        params['strength'] = params[\"strength_previous_frame\"]   \n",
        "\n",
        "\n",
        "      elif params[\"animation_mode\"] == \"2D\":\n",
        "        display.clear_output(wait=True)\n",
        "        if frame_num > 0:\n",
        "          params[\"use_init\"] = True\n",
        "          params[\"init_image\"] = os.path.join(outpath, f\"samples/{timestring}_{(base_count-1):04}_seed_{local_seed2}.png\")\n",
        "          params['strength'] = params[\"strength_previous_frame\"]\n",
        "          local_seed2 += 1000\n",
        "\n",
        "          img_0 = cv2.imread(params[\"init_image\"])\n",
        "\n",
        "          if params[\"key_frames\"]:\n",
        "            angle = angle_series[frame_num]\n",
        "            zoom = zoom_series[frame_num]\n",
        "            translation_x = translation_x_series[frame_num]\n",
        "            translation_y = translation_y_series[frame_num]\n",
        "            print(\n",
        "                f'angle: {angle}',\n",
        "                f'zoom: {zoom}',\n",
        "                f'translation_x: {translation_x}',\n",
        "                f'translation_y: {translation_y}',\n",
        "            )\n",
        "          \n",
        "          center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
        "          trans_mat = np.float32(\n",
        "              [[1, 0, translation_x],\n",
        "              [0, 1, translation_y]]\n",
        "          )\n",
        "          rot_mat = cv2.getRotationMatrix2D( center, angle, zoom )\n",
        "          trans_mat = np.vstack([trans_mat, [0,0,1]])\n",
        "          rot_mat = np.vstack([rot_mat, [0,0,1]])\n",
        "          transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
        "          img_0 = cv2.warpPerspective(\n",
        "              img_0,\n",
        "              transformation_matrix,\n",
        "              (img_0.shape[1], img_0.shape[0]),\n",
        "              borderMode=cv2.BORDER_WRAP\n",
        "          )\n",
        "          cv2.imwrite('prevFrameScaled.png', img_0)\n",
        "          params[\"init_image\"] = 'prevFrameScaled.png'\n",
        "          \n",
        "\n",
        "      # init image\n",
        "      if params[\"use_init\"]:\n",
        "        assert os.path.isfile(params[\"init_image\"])\n",
        "\n",
        "        init_image = load_img(params[\"init_image\"], params[\"W\"], params[\"H\"]).to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "        sampler.make_schedule(ddim_num_steps=params['ddim_steps'], ddim_eta=params['ddim_eta'], verbose=False)\n",
        "    \n",
        "        assert 0. <= params['strength'] <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "        t_enc = int(params['strength'] * params['ddim_steps'])\n",
        "        print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "      # no init image\n",
        "      else:\n",
        "        if params[\"fixed_code\"]:\n",
        "            start_code = torch.randn([params[\"n_samples\"], params[\"C\"], params[\"H\"] // params[\"f\"], params[\"W\"] // params[\"f\"]], device=device)\n",
        "\n",
        "      precision_scope = autocast if params[\"precision\"]==\"autocast\" else nullcontext\n",
        "      with torch.no_grad():\n",
        "          with precision_scope(\"cuda\"):\n",
        "              with model.ema_scope():\n",
        "                  tic = time.time()\n",
        "                  all_samples = list()\n",
        "                  #for n in trange(params[\"n_iter\"], desc=\"Sampling\"):\n",
        "                  for prompts in data:\n",
        "                      all_samples = list()\n",
        "                      grid_count = len(os.listdir(outpath)) - 1\n",
        "                      for n in range(params[\"n_iter\"]):\n",
        "                          local_seed2 = local_seed + n\n",
        "                          seed_everything(local_seed2)\n",
        "                          \n",
        "                          if params[\"sampler\"]==\"plms\":\n",
        "                            sampler = PLMSSampler(model)\n",
        "                            if params[\"use_init\"]:\n",
        "                              sampler.make_schedule(ddim_num_steps=params['ddim_steps'], ddim_eta=params['ddim_eta'], verbose=False)\n",
        "                          #for prompts in tqdm(data, desc=\"data\"):\n",
        "                          uc = None\n",
        "                          if params[\"scale\"] != 1.0:\n",
        "                              uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                          if isinstance(prompts, tuple):\n",
        "                              prompts = list(prompts)\n",
        "                          c = model.get_learned_conditioning(prompts)\n",
        "    #sampler = \"ddim\" # @param [\"ddim\",\"plms\",\"k_lms\",\"dpm_2\",\"dpm_2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\"] \n",
        "\n",
        "                          if params[\"sampler\"] in [\"k_lms\",\"dpm_2\",\"dpm_2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\"]:\n",
        "                            shape = [params[\"C\"], params[\"H\"] // params[\"f\"], params[\"W\"] // params[\"f\"]]\n",
        "                            sigmas = model_wrap.get_sigmas(params[\"ddim_steps\"])\n",
        "                            torch.manual_seed(local_seed2)\n",
        "                            x = torch.randn([params[\"n_samples\"], *shape], device=device) * sigmas[0]\n",
        "                            model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "                            extra_args = {'cond': c, 'uncond': uc, 'cond_scale': params[\"scale\"]}\n",
        "                            if params[\"sampler\"]==\"k_lms\":\n",
        "                              samples_ddim = K.sampling.sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                            elif params[\"sampler\"]==\"dpm_2\":\n",
        "                              samples_ddim = K.sampling.sample_dpm_2(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                            elif params[\"sampler\"]==\"dpm_2_ancestral\":\n",
        "                              samples_ddim = K.sampling.sample_dpm_2_ancestral(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                            elif params[\"sampler\"]==\"heun\":\n",
        "                              samples_ddim = K.sampling.sample_heun(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                            elif params[\"sampler\"]==\"euler\":\n",
        "                              samples_ddim = K.sampling.sample_euler(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                            elif params[\"sampler\"]==\"euler_ancestral\":\n",
        "                              samples_ddim = K.sampling.sample_euler_ancestral(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                              \n",
        "                            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                            x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                            x_samples_ddim = accelerator.gather(x_samples_ddim)\n",
        "                            x_samples = x_samples_ddim\n",
        "\n",
        "                          else:\n",
        "\n",
        "                            # no init image\n",
        "                            if not params['use_init']:\n",
        "                                shape = [params[\"C\"], params[\"H\"] // params[\"f\"], params[\"W\"] // params[\"f\"]]\n",
        "\n",
        "                                samples_ddim, _ = sampler.sample(S=params[\"ddim_steps\"],\n",
        "                                                                conditioning=c,\n",
        "                                                                batch_size=params[\"n_samples\"],\n",
        "                                                                shape=shape,\n",
        "                                                                verbose=False,\n",
        "                                                                unconditional_guidance_scale=params[\"scale\"],\n",
        "                                                                unconditional_conditioning=uc,\n",
        "                                                                eta=params[\"ddim_eta\"],\n",
        "                                                                x_T=start_code)\n",
        "\n",
        "                                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                                x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                                x_samples = x_samples_ddim\n",
        "\n",
        "                            # init image\n",
        "                            else:\n",
        "                              # encode (scaled latent)\n",
        "                              z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                              # decode it\n",
        "                              samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=params['scale'],\n",
        "                                                      unconditional_conditioning=uc,)\n",
        "      \n",
        "                              x_samples = model.decode_first_stage(samples)\n",
        "                              x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                          \n",
        "                          for x_sample in x_samples:\n",
        "                              x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                              if params[\"display_samples\"]:\n",
        "                                  display.display(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "                              if not params[\"skip_save\"]:\n",
        "                                  if params[\"animation_mode\"] != \"None\":\n",
        "                                    Image.fromarray(x_sample.astype(np.uint8)).save(\n",
        "                                        os.path.join(outpath, f\"samples/{timestring}_{base_count:04}_seed_{local_seed}.png\"))\n",
        "                                  else:\n",
        "                                    Image.fromarray(x_sample.astype(np.uint8)).save(\n",
        "                                        os.path.join(outpath, f\"samples/{timestring}_{base_count:04}_seed_{local_seed2}.png\"))\n",
        "                              base_count += 1\n",
        "\n",
        "                          if not params[\"skip_grid\"]:\n",
        "                              all_samples.append(x_samples)\n",
        "\n",
        "                      if not params[\"skip_grid\"]:\n",
        "                          # additionally, save as grid\n",
        "                          grid = torch.stack(all_samples, 0)\n",
        "                          grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                          grid = make_grid(grid, nrow=n_rows,padding=0)\n",
        "\n",
        "                          # to image\n",
        "                          grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                          if params[\"display_grid\"]:\n",
        "                              display.display(Image.fromarray(grid.astype(np.uint8)))\n",
        "                          Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'{timestring}_{grid_count:04}_grid.png'))\n",
        "                          #grid_count += 1\n",
        "\n",
        "                  toc = time.time()\n",
        "\n",
        "\n",
        "    #Animation - Create Video\n",
        "    # import subprocess in case this cell is run without the above cells\n",
        "    if  params[\"animation_mode\"] != \"None\":\n",
        "      import subprocess\n",
        "      from base64 import b64encode\n",
        "      init_frame = 1\n",
        "      fps = 12\n",
        "      last_frame = params[\"max_frames\"]\n",
        "      image_path = f\"{outpath}/samples/{timestring}_%04d_seed_{local_seed}.png\"\n",
        "      file_path = os.path.join(outpath, f'{timestring}.mp4')\n",
        "      cmd = [\n",
        "        'ffmpeg',\n",
        "        '-y',\n",
        "        '-vcodec',\n",
        "        'png',\n",
        "        '-r',\n",
        "        str(fps),\n",
        "        '-start_number',\n",
        "        str(init_frame),\n",
        "        '-i',\n",
        "        image_path,\n",
        "        '-frames:v',\n",
        "        str(last_frame+1),\n",
        "        '-c:v',\n",
        "        'libx264',\n",
        "        '-vf',\n",
        "        f'fps={fps}',\n",
        "        '-pix_fmt',\n",
        "        'yuv420p',\n",
        "        '-crf',\n",
        "        '17',\n",
        "        '-preset',\n",
        "        'veryslow',\n",
        "        file_path\n",
        "      ]\n",
        "\n",
        "      process = subprocess.Popen(cmd, cwd=f'{outpath}/samples', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "      stdout, stderr = process.communicate()\n",
        "      if process.returncode != 0:\n",
        "          print(stderr)\n",
        "          raise RuntimeError(stderr)\n",
        "      else:\n",
        "          print(\"The video is ready and saved to the images folder\")\n",
        "\n",
        "\n",
        "    #print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\" f\" \\nEnjoy.\")\n",
        "\n",
        "\n",
        "def parse_key_frames(string, prompt_parser=None):\n",
        "    \"\"\"Given a string representing frame numbers paired with parameter values at that frame,\n",
        "    return a dictionary with the frame numbers as keys and the parameter values as the values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    string: string\n",
        "        Frame numbers paired with parameter values at that frame number, in the format\n",
        "        'framenumber1: (parametervalues1), framenumber2: (parametervalues2), ...'\n",
        "    prompt_parser: function or None, optional\n",
        "        If provided, prompt_parser will be applied to each string of parameter values.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Frame numbers as keys, parameter values at that frame number as values\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    RuntimeError\n",
        "        If the input string does not match the expected format.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\")\n",
        "    {10: 'Apple: 1| Orange: 0', 20: 'Apple: 0| Orange: 1| Peach: 1'}\n",
        "\n",
        "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\", prompt_parser=lambda x: x.lower()))\n",
        "    {10: 'apple: 1| orange: 0', 20: 'apple: 0| orange: 1| peach: 1'}\n",
        "    \"\"\"\n",
        "    import re\n",
        "    pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
        "    frames = dict()\n",
        "    for match_object in re.finditer(pattern, string):\n",
        "        frame = int(match_object.groupdict()['frame'])\n",
        "        param = match_object.groupdict()['param']\n",
        "        if prompt_parser:\n",
        "            frames[frame] = prompt_parser(param)\n",
        "        else:\n",
        "            frames[frame] = param\n",
        "\n",
        "    if frames == {} and len(string) != 0:\n",
        "        raise RuntimeError('Key Frame string not correctly formatted')\n",
        "    return frames\n",
        "\n",
        "def get_inbetweens(key_frames, integer=False):\n",
        "\n",
        "    \"\"\"Given a dict with frame numbers as keys and a parameter value as values,\n",
        "    return a pandas Series containing the value of the parameter at every frame from 0 to max_frames.\n",
        "    Any values not provided in the input dict are calculated by linear interpolation between\n",
        "    the values of the previous and next provided frames. If there is no previous provided frame, then\n",
        "    the value is equal to the value of the next provided frame, or if there is no next provided frame,\n",
        "    then the value is equal to the value of the previous provided frame. If no frames are provided,\n",
        "    all frame values are NaN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    key_frames: dict\n",
        "        A dict with integer frame numbers as keys and numerical values of a particular parameter as values.\n",
        "    integer: Bool, optional\n",
        "        If True, the values of the output series are converted to integers.\n",
        "        Otherwise, the values are floats.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        A Series with length max_frames representing the parameter values for each frame.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> max_frames = 5\n",
        "    >>> get_inbetweens({1: 5, 3: 6})\n",
        "    0    5.0\n",
        "    1    5.0\n",
        "    2    5.5\n",
        "    3    6.0\n",
        "    4    6.0\n",
        "    dtype: float64\n",
        "\n",
        "    >>> get_inbetweens({1: 5, 3: 6}, integer=True)\n",
        "    0    5\n",
        "    1    5\n",
        "    2    5\n",
        "    3    6\n",
        "    4    6\n",
        "    dtype: int64\n",
        "    \"\"\"\n",
        "    key_frame_series = pd.Series([np.nan for a in range(params[\"max_frames\"])])\n",
        "\n",
        "    for i, value in key_frames.items():\n",
        "        key_frame_series[i] = value\n",
        "    key_frame_series = key_frame_series.astype(float)\n",
        "    \n",
        "    print(params[\"interp_spline\"])\n",
        "    interp_method = params[\"interp_spline\"]\n",
        "\n",
        "    if interp_method == 'Cubic' and len(key_frames.items()) <=3:\n",
        "      interp_method = 'Quadratic'\n",
        "    \n",
        "    if interp_method == 'Quadratic' and len(key_frames.items()) <= 2:\n",
        "      interp_method = 'Linear'\n",
        "      \n",
        "    \n",
        "    key_frame_series[0] = key_frame_series[key_frame_series.first_valid_index()]\n",
        "    key_frame_series[params[\"max_frames\"]-1] = key_frame_series[key_frame_series.last_valid_index()]\n",
        "    # key_frame_series = key_frame_series.interpolate(method=intrp_method,order=1, limit_direction='both')\n",
        "    key_frame_series = key_frame_series.interpolate(method=interp_method.lower(),limit_direction='both')\n",
        "    if integer:\n",
        "        return key_frame_series.astype(int)\n",
        "    return key_frame_series\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIUJ7lWI4v53",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **Model Path Variables**\n",
        "# ask for the link\n",
        "download_link = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# config\n",
        "if os.path.exists(models_path+'/v1-inference.yaml'):\n",
        "  print(f\"{models_path+'/v1-inference.yaml'} exists\")\n",
        "else:\n",
        "  print(\"cp ./stable-diffusion/configs/stable-diffusion/v1-inference.yaml $models_path/.\")\n",
        "  !cp ./stable-diffusion/configs/stable-diffusion/v1-inference.yaml $models_path/\n",
        "\n",
        "\n",
        "# weights \n",
        "if os.path.exists(models_path+'/sd-v1-3-full-ema.ckpt'):\n",
        "  print(f\"{models_path+'/sd-v1-3-full-ema.ckpt'} exists\")\n",
        "else:\n",
        "  print(f\"!wget -O $models_path/sd-v1-3-full-ema.ckpt {download_link}\")\n",
        "  !wget -O $models_path/sd-v1-3-full-ema.ckpt $download_link\n",
        "\n",
        "config = models_path+'/v1-inference.yaml'\n",
        "ckpt = models_path+'/sd-v1-3-full-ema.ckpt'\n",
        "\n",
        "print(f\"config: {config}\")\n",
        "print(f\"ckpt: {ckpt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJiMgz_96nr3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **Load Stable Diffusion**\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False, device='cuda'):\n",
        "    map_location = \"cuda\" #@param [\"cpu\", \"cuda\"]\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=map_location)\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    #model.cuda()\n",
        "    model = model.half().to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "load_on_run_all = False #@param {type: 'boolean'}\n",
        "\n",
        "if load_on_run_all:\n",
        "  local_config = OmegaConf.load(f\"{config}\")\n",
        "  model = load_model_from_config(local_config, f\"{ckpt}\")\n",
        "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "  model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov3r4RD1tzsT"
      },
      "source": [
        "# **Run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qH74gBWDd2oq"
      },
      "outputs": [],
      "source": [
        "def opt_params():\n",
        "  \n",
        "  #@markdown **Save & Display Settings**\n",
        "  batchdir = \"test\" #@param {type:\"string\"}\n",
        "  outdir = get_output_folder(output_path,batchdir)\n",
        "  save_settings = True #@param {type:\"boolean\"}\n",
        "  skip_save = False #@param {type:\"boolean\"}\n",
        "  skip_grid = False #@param {type:\"boolean\"}\n",
        "  display_grid = True #@param {type:\"boolean\"}\n",
        "  display_samples = True #@param {type:\"boolean\"}\n",
        "\n",
        "  #@markdown **Prompt Settings**\n",
        "  seed = 42000 #@param\n",
        "\n",
        "  ##@markdown *One prompt by line*\n",
        "  #prompts = \"a forest by asher brown durand\" #@param {type:\"raw\"}\n",
        "\n",
        "  #@markdown **Image Settings**\n",
        "  n_samples = 1 #@param\n",
        "  n_rows = 2 #@param\n",
        "  W = 512 #@param\n",
        "  H = 512 #@param\n",
        "\n",
        "  #@markdown **Init Settings**\n",
        "  use_init = False #@param {type:\"boolean\"}\n",
        "  init_image = \"videoFrames/0001.jpg\" #@param {type:\"string\"}\n",
        "  strength = 0.5 #@param {type:\"number\"}\n",
        "  \n",
        "  #@markdown **Sampling Settings**\n",
        "  scale = 10 #@param\n",
        "  n_iter = 4 #@param\n",
        "  ddim_steps = 50 #@param\n",
        "  ddim_eta = 1.0 #@param\n",
        "  #plms = False #@param {type:\"boolean\"}\n",
        "  #k_lms = True #@param {type:\"boolean\"}\n",
        "\n",
        "  sampler = \"plms\" # @param [\"ddim\",\"plms\",\"k_lms\",\"dpm_2\",\"dpm_2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\"] \n",
        "  \n",
        "  #@markdown **Batch Settings**\n",
        "  n_batch = 1 #@param\n",
        "\n",
        "  #@markdown **no idea what this does**\n",
        "  precision = 'autocast' #@param\n",
        "  fixed_code = True #@param\n",
        "  C = 4 #@param\n",
        "  f = 8 #@param\n",
        "\n",
        "\n",
        "  #@markdown **Animation Settings**\n",
        "\n",
        "#@markdown ####**Animation Mode:**\n",
        "  animation_mode = 'None' #@param ['None', '2D', 'Video Input'] {type:'string'}\n",
        "  #@markdown IMPORTANT: For animation you must choose ddim or plms as sampler\n",
        "  ### code must be changed to accept other sampler (issue with the \"skip steps\")\n",
        "  \n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown ####**Video Input Settings:**\n",
        "\n",
        "  video_init_path = \"/mnt/c/Users/Thiba/Downloads/output_colab/test_video.mp4\" #@param {type: 'string'}\n",
        "  extract_nth_frame = 1 #@param {type: 'number'}\n",
        "  video_init_seed_continuity = True #@param {type: 'boolean'}\n",
        "\n",
        "  if animation_mode == \"Video Input\":\n",
        "    if mount_google_drive:\n",
        "      videoFramesFolder = f'/content/videoFrames'\n",
        "    else:\n",
        "      videoFramesFolder = f'videoFrames'\n",
        "    os.makedirs(videoFramesFolder, exist_ok=True)\n",
        "    print(f\"Exporting Video Frames (1 every {extract_nth_frame})...\")\n",
        "    try:\n",
        "      for f in pathlib.Path(f'{videoFramesFolder}').glob('*.jpg'):\n",
        "        f.unlink()\n",
        "    except:\n",
        "      print('')\n",
        "    vf = f'select=not(mod(n\\,{extract_nth_frame}))'\n",
        "    subprocess.run(['ffmpeg', '-i', f'{video_init_path}', '-vf', f'{vf}', '-vsync', 'vfr', '-q:v', '2', '-loglevel', 'error', '-stats', f'{videoFramesFolder}/%04d.jpg'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown ####**2D Animation Settings:**\n",
        "  #@markdown `zoom` is a multiplier of dimensions, 1 is no zoom.\n",
        "  #@markdown All rotations are provided in degrees.\n",
        "\n",
        "  key_frames = True #@param {type:\"boolean\"}\n",
        "  max_frames = 48#@param {type:\"number\"}\n",
        "  \n",
        "  if animation_mode == \"Video Input\":\n",
        "    max_frames = len(glob(f'{videoFramesFolder}/*.jpg'))\n",
        "\n",
        "  interp_spline = 'Linear' #Do not change, currently will not look good. param ['Linear','Quadratic','Cubic']{type:\"string\"}\n",
        "  angle = \"0:(0)\"#@param {type:\"string\"}\n",
        "  zoom = \"0: (1.0)\"#@param {type:\"string\"}\n",
        "  translation_x = \"0: (0)\"#@param {type:\"string\"}\n",
        "  translation_y = \"0: (0)\"#@param {type:\"string\"}\n",
        "  near_plane = 200#@ param {type:\"number\"}\n",
        "  far_plane = 10000#@ param {type:\"number\"}\n",
        "  fov = 40# @ param {type:\"number\"}\n",
        "  padding_mode = 'border'#@ param {type:\"string\"}\n",
        "  sampling_mode = 'bicubic'#@ param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown ####**Coherency Settings:**\n",
        "  #@markdown `strength_previous_frame` higher value change more the frame\n",
        "  strength_previous_frame = 0.50 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "  return locals()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "#\"a beautiful forest by Asher Brown Durand, trending on Artstation\", #the first prompt I want\n",
        "\"a beautiful portrait of a goddess, by Artgerm and Mucha and Greg Rutkowski, trending on Artstation\", #the second prompt I want\n",
        "#[\"the third prompt I don't want it I commented it with an #\"],\n",
        "]"
      ],
      "metadata": {
        "id": "2ujwkGZTcGev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxx8BzxjiaXg"
      },
      "outputs": [],
      "source": [
        "#@markdown **Run**\n",
        "params = opt_params()\n",
        "params[\"filename\"] = None\n",
        "params[\"prompts\"] = prompts\n",
        "for ii in range(params[\"n_batch\"]):\n",
        "  num = params[\"n_batch\"]\n",
        "  print(f\"run {ii+1} of {num}\")\n",
        "  run(params)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Rock n Roll Diffusion",
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}